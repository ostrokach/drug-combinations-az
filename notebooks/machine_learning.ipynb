{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimlab1/strokach/anaconda3/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "from biodata import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from az_dream import functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from common import dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    'scaled': {'raw': pd.read_hdf('machine_learning/ddc_data_scaled.h5')},\n",
    "    'pca': {'raw': pd.read_hdf('machine_learning/ddc_data_pca.h5')},\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunks = dict()\n",
    "chunk_size = 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_chunks(df_train):\n",
    "    chunks = []\n",
    "    df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
    "    for i in range(0, df_train.shape[0], chunk_size):\n",
    "        train = pd.concat([df_train[0:i], df_train[i + chunk_size:]], ignore_index=True).copy()\n",
    "        test = df_train[i:i + chunk_size].copy()\n",
    "        #print(len(train))\n",
    "        #print(len(test))\n",
    "\n",
    "        # Make sure there is no same (d_1, d_2, c) in train and test\n",
    "        test['unique_id'] = test[['d_1', 'd_2', 'c']].apply('.'.join, axis=1)\n",
    "        train['unique_id'] = train[['d_1', 'd_2', 'c']].apply('.'.join, axis=1)\n",
    "        test_unique_id = set(test['unique_id'])\n",
    "        train = train[~train['unique_id'].isin(test_unique_id)]\n",
    "        train_unique_id = set(train['unique_id'])\n",
    "        #print(len(train))\n",
    "        #print(len(test))\n",
    "        assert not test_unique_id & train_unique_id, test_unique_id & train_unique_id\n",
    "\n",
    "        # Make sure there are some (d_1, d_2) in train and test\n",
    "        test_drug_pair = set(test[['d_1', 'd_2']].apply('.'.join, axis=1))\n",
    "        train_drug_pair = set(train[['d_1', 'd_2']].apply('.'.join, axis=1))\n",
    "        assert len(test_drug_pair & train_drug_pair) + 4 >= len(test_drug_pair), \\\n",
    "            (len(test_drug_pair & train_drug_pair), len(test_drug_pair))\n",
    "\n",
    "        assert not train['synergy_score'].isnull().any()\n",
    "        assert not test['synergy_score'].isnull().any()\n",
    "        chunks.append((train, test))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for feature_format in ['scaled', 'pca']:\n",
    "    input_df = input_data[feature_format]['raw']\n",
    "    \n",
    "    df_train = (\n",
    "        input_df[\n",
    "            (input_df['source'] == 'train') | \n",
    "            (input_df['source'] == 'ch2_validate')\n",
    "        ]\n",
    "    ).copy()\n",
    "\n",
    "    df_validate = (\n",
    "        input_df[\n",
    "            (input_df['source'] == 'ch1_validate')\n",
    "        ]\n",
    "    ).copy()\n",
    "\n",
    "    chunks = get_chunks(df_train)\n",
    "    \n",
    "    input_data[feature_format]['train'] = df_train\n",
    "    input_data[feature_format]['train_chunks'] = chunks\n",
    "    input_data[feature_format]['validate'] = df_validate\n",
    "    # input_data[feature_format]['train_chunks'] = get_chunks(input_data[feature_format]['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "assert (df_train['d_1'].str.lower() <= df_train['d_2'].str.lower()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_COMPONENTS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def model(data, train=False):\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    tf.matmul(hidden, fc2_weights) + fc2_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_this_feature_columns(feature_format):    \n",
    "    print(\"Input type: {}\".format(feature_format))\n",
    "    if feature_format == 'pca':\n",
    "        this_feature_columns = list(range(N_COMPONENTS))\n",
    "    else:\n",
    "        this_feature_columns = feature_columns.copy()\n",
    "    return this_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "\n",
    "batch_size = 128\n",
    "layer_1_size = 32\n",
    "num_labels = 1\n",
    "\n",
    "n_rows = None\n",
    "n_columns = train_dataset.shape[1]\n",
    "#n_columns = len(get_this_feature_columns(feature_format))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(42)\n",
    "    \n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(n_rows, n_columns))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(n_rows, num_labels))\n",
    "    tf_valid_dataset = tf.placeholder(tf.float32, shape=(n_rows, n_columns))\n",
    "    # tf_test_dataset = tf.placeholder(tf.float32, shape=(test_df.shape[0], len(feature_columns)))\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([n_columns, layer_1_size], stddev=1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([layer_1_size]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([layer_1_size, num_labels], stddev=1))\n",
    "    layer2_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    #layer3_weights = tf.Variable(tf.truncated_normal([256, num_labels]))\n",
    "    #layer3_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    def model(data, train=False):\n",
    "        hidden = tf.matmul(data, layer1_weights) + layer1_biases\n",
    "        hidden = tf.nn.relu6(hidden)\n",
    "        if train:\n",
    "            hidden = tf.nn.dropout(hidden, 0.5)\n",
    "        hidden = tf.matmul(hidden, layer2_weights) + layer2_biases\n",
    "\n",
    "        #hidden = tf.matmul(hidden, layer3_weights) + layer3_biases\n",
    "        return hidden\n",
    "\n",
    "    def correlation(X, Y):\n",
    "        X_mean = tf.reduce_mean(X)\n",
    "        Y_mean = tf.reduce_mean(Y)\n",
    "        X_std = tf.sqrt(tf.reduce_sum(tf.square(X - X_mean)))\n",
    "        Y_std = tf.sqrt(tf.reduce_sum(tf.square(Y - Y_mean)))\n",
    "        cov = tf.matmul(tf.transpose(X - X_mean), (Y - Y_mean))\n",
    "        corr = cov / (X_std * Y_std)\n",
    "        return tf.squeeze(corr)\n",
    "        \n",
    "    \n",
    "    logits = model(tf_train_dataset, train=True)\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    #weights = (\n",
    "    #    0.8 * tf.to_float(tf_train_labels > 20) + \n",
    "    #    0.2 * tf.to_float(tf_train_labels <= 20)\n",
    "    #)\n",
    "    #loss = tf.reduce_mean(tf.square(logits - tf_train_labels) * weights)\n",
    "    #loss = tf.reduce_mean(tf.square(logits - tf_train_labels))\n",
    "    loss = -correlation(logits, tf_train_labels)\n",
    "    \n",
    "    # Add regularization\n",
    "    regularizers = (\n",
    "        tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) +\n",
    "        tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases)\n",
    "        #tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases)\n",
    "    )\n",
    "    loss += 1e-5 * regularizers\n",
    "\n",
    "    # Optimizer.\n",
    "    step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, 1, 0.999)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)  # global_step=step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = logits\n",
    "    valid_prediction = model(tf_valid_dataset)\n",
    "    # test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import Imputer, StandardScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "imputer = Imputer(strategy='median')\n",
    "standard_scaler = StandardScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "maxabs_scaler = MaxAbsScaler()\n",
    "pca = PCA()\n",
    "\n",
    "scale_factor = 0.01\n",
    "\n",
    "def get_data_for_step(feature_format, train_df, test_df):\n",
    "    this_feature_columns = get_this_feature_columns(feature_format)\n",
    "    \n",
    "    if False:\n",
    "        new_features = []\n",
    "        \n",
    "#         pca.fit(np.vstack([\n",
    "#             train_sg[this_feature_columns + new_features].values, \n",
    "#             test_sg[this_feature_columns + new_features].values]))\n",
    "#         train_dataset = (\n",
    "#             pca.transform(train_df[this_feature_columns + new_features].values)[:, :N_COMPONENTS]\n",
    "#         )\n",
    "#         valid_dataset = (\n",
    "#             pca.transform(test_df[this_feature_columns + new_features].values)[:, :N_COMPONENTS]\n",
    "#         )\n",
    "                \n",
    "        train_dataset = train_df[this_feature_columns].values\n",
    "        valid_dataset = test_df[this_feature_columns].values\n",
    "\n",
    "    else:\n",
    "        train_sg = fn.get_synergy_groups(train_df, train_df.drop('synergy_score', axis=1), pairwise=False)\n",
    "        test_sg = fn.get_synergy_groups(train_df, test_df.drop('synergy_score', axis=1), pairwise=False)\n",
    "\n",
    "        new_features = [\n",
    "            c for c in train_sg.columns \n",
    "            if any([isinstance(c, str) and c.startswith(prefix) for prefix in ['count_', 'synergy_score_']])\n",
    "        ]\n",
    "        print(new_features)\n",
    "\n",
    "        assert not any([\n",
    "            c in (this_feature_columns + new_features)\n",
    "            for c in ['synergy_score', 'synergy_score_diff']\n",
    "        ])\n",
    "\n",
    "        imputer.fit(np.vstack([train_sg[new_features].values, test_sg[new_features].values]))\n",
    "        train_sg.loc[:, new_features] = imputer.transform(train_sg[new_features].values)\n",
    "        test_sg.loc[:, new_features] = imputer.transform(test_sg[new_features].values)\n",
    "        \n",
    "        robust_scaler.fit(np.vstack([train_sg[new_features].values, test_sg[new_features].values]))\n",
    "        train_sg.loc[:, new_features] = robust_scaler.transform(train_sg[new_features].values)\n",
    "        test_sg.loc[:, new_features] = robust_scaler.transform(test_sg[new_features].values)\n",
    "\n",
    "        maxabs_scaler.fit(np.vstack([train_sg[new_features].values, test_sg[new_features].values]))\n",
    "        train_sg.loc[:, new_features] = maxabs_scaler.transform(train_sg[new_features].values)\n",
    "        test_sg.loc[:, new_features] = maxabs_scaler.transform(test_sg[new_features].values)\n",
    "\n",
    "        if True:\n",
    "            pca.fit(np.vstack([\n",
    "                train_sg[this_feature_columns + new_features].values, \n",
    "                test_sg[this_feature_columns + new_features].values]))\n",
    "            train_dataset = (\n",
    "                pca.transform(train_sg[this_feature_columns + new_features].values)[:, :N_COMPONENTS]\n",
    "            )\n",
    "            valid_dataset = (\n",
    "                pca.transform(test_sg[this_feature_columns + new_features].values)[:, :N_COMPONENTS]\n",
    "            )\n",
    "        else:\n",
    "            train_dataset = train_sg[this_feature_columns + new_features].values\n",
    "            valid_dataset = test_sg[this_feature_columns + new_features].values\n",
    "            \n",
    "        assert (train_df[['d_1', 'd_2', 'c']] == train_sg[['d_1', 'd_2', 'c']]).all().all()\n",
    "        assert (test_df[['d_1', 'd_2', 'c']] == test_sg[['d_1', 'd_2', 'c']]).all().all()\n",
    "        \n",
    "    train_labels = train_df[['synergy_score']].values * scale_factor\n",
    "    valid_labels = test_df[['synergy_score']].values * scale_factor\n",
    "    \n",
    "    return train_dataset, train_labels, valid_dataset, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type: pca\n",
      "['synergy_score_gbdd_median', 'count_gbdd', 'synergy_score_gbc_median', 'count_gbc', 'synergy_score_gbdc_median_mean', 'synergy_score_gbdc_median_diff', 'count_gbdc_mean', 'count_gbdc_diff', 'synergy_score_gbd_median_mean', 'synergy_score_gbd_median_diff', 'count_gbd_mean', 'count_gbd_diff']\n",
      "Initialized\n",
      "Step: 0 (1)\n",
      "Minibatch loss at step 0: 0.1291968822479248\n",
      "Minibatch correlation: [ 0.09575372]\n",
      "Validation correlation: [ 0.16231606]\n",
      "Skipped 0 out of 167 groups...\n",
      "Validation score: (0.11943610009946184, 0.084153423464882485)\n",
      "\n",
      "Step: 20 (1)\n",
      "Minibatch loss at step 20: -0.8679263591766357\n",
      "Minibatch correlation: [ 0.44471968]\n",
      "Validation correlation: [ 0.2681028]\n",
      "Skipped 0 out of 167 groups...\n",
      "Validation score: (0.26024795117766553, 0.32079666138242707)\n",
      "\n",
      "Step: 40 (1)\n",
      "Minibatch loss at step 40: -0.7014860510826111\n",
      "Minibatch correlation: [ 0.54281249]\n",
      "Validation correlation: [ 0.25529122]\n",
      "Skipped 0 out of 167 groups...\n",
      "Validation score: (0.28367252535189696, 0.37101074889660091)\n",
      "\n",
      "Step: 60 (1)\n",
      "Minibatch loss at step 60: -0.8737273216247559\n",
      "Minibatch correlation: [ 0.56235493]\n",
      "Validation correlation: [ 0.23871953]\n",
      "Skipped 0 out of 167 groups...\n",
      "Validation score: (0.30530772330868522, 0.38122258482380528)\n",
      "\n",
      "Step: 80 (1)\n",
      "Minibatch loss at step 80: -0.8466054797172546\n",
      "Minibatch correlation: [ 0.57602383]\n",
      "Validation correlation: [ 0.21731008]\n",
      "Skipped 0 out of 167 groups...\n",
      "Validation score: (0.31032534577595206, 0.36772921946707032)\n",
      "\n",
      "Step: 100 (1)\n",
      "Minibatch loss at step 100: -0.8397253155708313\n",
      "Minibatch correlation: [ 0.5838419]\n",
      "Validation correlation: [ 0.20122113]\n",
      "Skipped 0 out of 167 groups...\n",
      "Validation score: (0.29398713524718795, 0.35075632314910277)\n",
      "\n",
      "Number of groups: 537\n",
      "Skipped 1 groups\n"
     ]
    }
   ],
   "source": [
    "num_steps = 103\n",
    "output_dfs = []\n",
    "feature_format = 'pca'\n",
    "\n",
    "# for train_df, test_df in input_data[feature_format]['train_chunks']:\n",
    "for train_df, test_df in [(input_data[feature_format]['train'], input_data[feature_format]['validate']), ]:\n",
    "    \n",
    "    train_df.index = range(len(train_df))\n",
    "    test_df.index = range(len(test_df))\n",
    "    grouped = shuffle(list(train_df.groupby(['d_1', 'd_2'])))\n",
    "    grouped_c = shuffle(list(train_df.groupby(['c'])))\n",
    "    \n",
    "    train_dataset, train_labels, valid_dataset, valid_labels = (\n",
    "        get_data_for_step(feature_format, train_df, test_df)\n",
    "    )\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            batch_predictions_all = []\n",
    "            batch_labels_all = []\n",
    "            batch_losses = []\n",
    "\n",
    "            which = np.random.choice([0, 1, 2])\n",
    "            if (step % 50) in range(0, 3):\n",
    "                which = step % 50\n",
    "            \n",
    "            which = 1\n",
    "            \n",
    "            if which == 0:\n",
    "                train_dataset_this_step, train_labels_this_step = shuffle(train_dataset, train_labels)\n",
    "                # Collect predictions from every batch\n",
    "                for offset in range(0, train_dataset_this_step.shape[0] - batch_size , batch_size):\n",
    "                    batch_data = train_dataset_this_step[offset:(offset + batch_size), :]\n",
    "                    batch_labels = train_labels_this_step[offset:(offset + batch_size), :]\n",
    "                    feed_dict = {\n",
    "                        tf_train_dataset: batch_data, \n",
    "                        tf_train_labels: batch_labels,\n",
    "                    }\n",
    "                    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "                    #\n",
    "                    batch_predictions_all.append(predictions)\n",
    "                    batch_labels_all.append(batch_labels)\n",
    "                    batch_losses.append(l)\n",
    "            elif which == 1:              \n",
    "                grouped = shuffle(grouped)\n",
    "                n_skipped = 0\n",
    "                for key, group in grouped:\n",
    "                    if len(group) < 2:\n",
    "                        n_skipped += 1\n",
    "                        continue\n",
    "                    batch_data = train_dataset[group.index, :]\n",
    "                    batch_labels = train_labels[group.index, :]\n",
    "                    feed_dict = {\n",
    "                        tf_train_dataset: batch_data, \n",
    "                        tf_train_labels: batch_labels,\n",
    "                    }\n",
    "                    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "                    #\n",
    "                    batch_predictions_all.append(predictions)\n",
    "                    batch_labels_all.append(batch_labels)\n",
    "                    batch_losses.append(l)\n",
    "            elif which == 2:\n",
    "                grouped_c = shuffle(grouped_c)\n",
    "                n_skipped = 0\n",
    "                for key, group in grouped_c:\n",
    "                    if len(group) < 2:\n",
    "                        n_skipped += 1\n",
    "                        continue\n",
    "                    batch_data = train_dataset[group.index, :]\n",
    "                    batch_labels = train_labels[group.index, :]\n",
    "                    feed_dict = {\n",
    "                        tf_train_dataset: batch_data, \n",
    "                        tf_train_labels: batch_labels,\n",
    "                    }\n",
    "                    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "                    #\n",
    "                    batch_predictions_all.append(predictions)\n",
    "                    batch_labels_all.append(batch_labels)\n",
    "                    batch_losses.append(l)\n",
    "\n",
    "            step_predictions = np.vstack(batch_predictions_all)\n",
    "            step_labels = np.vstack(batch_labels_all)\n",
    "            step_loss = np.mean(batch_losses)\n",
    "            \n",
    "            if step % 20 == 0:\n",
    "                print(\"Step: {} ({})\".format(step, which))\n",
    "                print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
    "                if False:\n",
    "                    print(\"Minibatch accuracy: %.2f%%\" % accuracy(predictions, batch_labels))\n",
    "                    print(\"Validation accuracy: %.2f%%\" % accuracy(v_prediction, valid_labels))\n",
    "                    fpr, tpr, thresholds = roc_curve(valid_labels[:, 0], v_prediction[:, 0])\n",
    "                    bac = (tpr + (1 - fpr)) / 2\n",
    "                    print('bac: {:.2f}\\n'.format(max(bac)))\n",
    "                else:\n",
    "                    print(\"Minibatch correlation: {}\".format(sp.stats.pearsonr(step_predictions, step_labels)[0]))\n",
    "                    y_pred = valid_prediction.eval(feed_dict={tf_valid_dataset: valid_dataset})\n",
    "                    print(\"Validation correlation: {}\".format(sp.stats.pearsonr(y_pred, valid_labels)[0]))\n",
    "                    tmp = test_df.copy()\n",
    "                    tmp['synergy_score_pred'] = y_pred\n",
    "                    print(\"Validation score: {}\".format(fn.get_score_ch1(tmp)))\n",
    "                print('')\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "        #saver.save(session, 'machine_learning/tensorflow_ch1', global_step=step)\n",
    "        #print(\"Test accuracy: %.2f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    try:\n",
    "        print(\"Number of groups: {}\".format(len(grouped)))\n",
    "        print(\"Skipped {} groups\".format(n_skipped))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    output_df = test_df[['d_1', 'd_2', 'c', 'synergy_score']].copy()\n",
    "    output_df['synergy_score_pred'] = y_pred\n",
    "    output_dfs.append(output_df)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(valid_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_validate.copy()\n",
    "tmp['synergy_score_pred'] = y_pred\n",
    "fn.get_score_ch1(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
